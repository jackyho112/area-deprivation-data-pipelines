{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Summary\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scope the Project and Gather Data\n",
    "\n",
    "## Data\n",
    "\n",
    "- US Import data from [Amazon Data Exchange](https://aws.amazon.com/marketplace/pp/US-Imports-Automated-Manifest-System-AMS-Shipments/prodview-stk4wn3mbhx24) for shipping header, bill, cargo description, hazmat, hazmat class, tariff, cosignee, notified party, shipper, container, and mark number data for 2020, 2019, or 2018. \n",
    "\n",
    "- Harmonized Tariff data from the United States International Trade Commission [archive](https://www.usitc.gov/tata/hts/archive/index.htm)\n",
    "\n",
    "## Scope\n",
    "\n",
    "We will create the fact table by assembling the header dataset joined by the bill dataset from the data exchange repo. We will create the cargo dimension table by joining the cargo description, hazmat, hazmat class, tariff, and tariff harmonized number datasets. The last dataset will be from the US trade comission site, while the rest can be found at the exchange repo. We will create the contact table by concatenating the shipper, notified party, and cosignee datasets from AWS. Lastly, we could create the container table by joining the container and mark datasets from AWS. For use cases, we can visualize the final tables to explore the US import trends at different years, or do a linenar regression analysis to find relationships between columns. We could also build a time series data to predict future imports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Explore and Assess the Data\n",
    "\n",
    "Please refer to [the exploration notebook](https://github.com/jackyho112/us-import-data-pipelines/blob/main/notebooks/exploration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define the data model\n",
    "\n",
    "Please refer to the [output](https://github.com/jackyho112/us-import-data-pipelines#output-data) part of the readme file and all the references it contains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Run ETL to Model the Data\n",
    "\n",
    "- [Pipeline](https://github.com/jackyho112/us-import-data-pipelines#pipeline)\n",
    "- [Data validation](https://github.com/jackyho112/us-import-data-pipelines/blob/main/airflow/plugins/scripts/data_test.py)\n",
    "- [Data dictionary](https://github.com/jackyho112/us-import-data-pipelines/blob/main/data_dictionary.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: [Project Writeup](https://github.com/jackyho112/us-import-data-pipelines#project-write-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
